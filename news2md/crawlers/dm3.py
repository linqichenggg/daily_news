"""
3DMæ–°é—»çˆ¬è™« - ä½¿ç”¨ requests + BeautifulSoup çˆ¬å–3DMå•æœºæ¸¸æˆæ–°é—»
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List
import re

from .base import BaseCrawler, NewsPost


class DM3Crawler(BaseCrawler):
    """3DMå•æœºæ¸¸æˆæ–°é—»çˆ¬è™«"""
    
    # å•æœºèµ„è®¯é¡µé¢URL
    NEWS_URL = "https://www.3dmgame.com/news_32_1/"
    
    # æœ€å¤šæ”¶é›†å¤šå°‘æ¡æ–°é—»
    MAX_POSTS = 30
    
    USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    def __init__(self, config: dict = None):
        """åˆå§‹åŒ–çˆ¬è™«"""
        super().__init__("3DM")
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache, no-store, must-revalidate',
            'Pragma': 'no-cache',
            'Referer': 'https://www.3dmgame.com/',
        })
    
    def test_connection(self) -> bool:
        """æµ‹è¯•3DMç½‘ç«™è¿žæŽ¥"""
        try:
            response = self.session.get(self.NEWS_URL, timeout=10)
            if response.status_code == 200:
                print("âœ… 3DM è¿žæŽ¥æˆåŠŸï¼")
                return True
            else:
                print(f"âŒ 3DM è¿žæŽ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ 3DM è¿žæŽ¥å¤±è´¥: {e}")
            return False
    
    def crawl(self) -> List[NewsPost]:
        """çˆ¬å–3DMå•æœºæ¸¸æˆæ–°é—»ï¼ˆä»Žæœ€æ–°å¼€å§‹ï¼Œæœ€å¤š30æ¡ï¼‰"""
        print("ðŸ”¹ å¼€å§‹çˆ¬å–3DMå•æœºæ¸¸æˆæ–°é—»...")
        posts = []
        
        try:
            # æ·»åŠ æ—¶é—´æˆ³å‚æ•°ç»•è¿‡ CDN ç¼“å­˜
            import time
            cache_bust = f"?_t={int(time.time())}"
            response = self.session.get(self.NEWS_URL + cache_bust, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»æ¡ç›®ï¼ˆHTMLä¸­å·²æŒ‰æ—¶é—´å€’åºï¼Œæœ€æ–°åœ¨å‰ï¼‰
            news_items = soup.find_all('li', class_='selectpost')
            print(f"   é¡µé¢å…± {len(news_items)} æ¡æ–°é—»")
            
            # ä»Žæœ€æ–°å¼€å§‹ï¼Œæœ€å¤šå– MAX_POSTS æ¡
            for item in news_items[:self.MAX_POSTS]:
                try:
                    post = self._parse_news_item(item)
                    if post:
                        posts.append(post)
                        # æ‰“å°ç¬¬ä¸€æ¡æ–°é—»çš„æ—¶é—´ï¼ŒéªŒè¯æ˜¯å¦ä»Žæœ€æ–°å¼€å§‹
                        if len(posts) == 1:
                            print(f"   æœ€æ–°æ–°é—»: {post.title[30]}... ({post.published_at})")
                except Exception as e:
                    print(f"   âš ï¸ è§£æžæ–°é—»æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            print(f"   âœ… 3DMçˆ¬å–å®Œæˆï¼ŒèŽ·å– {len(posts)} æ¡æ–°é—»")
            
        except requests.RequestException as e:
            print(f"   âŒ è¯·æ±‚3DMå¤±è´¥: {e}")
        except Exception as e:
            print(f"   âŒ çˆ¬å–3DMå‡ºé”™: {e}")
        
        return posts
    
    def _parse_news_item(self, item) -> NewsPost:
        """è§£æžå•æ¡æ–°é—»"""
        # èŽ·å–æ ‡é¢˜å’Œé“¾æŽ¥
        title_tag = item.find('a', class_='bt')
        if not title_tag:
            return None
        
        title = title_tag.get_text(strip=True)
        url = title_tag.get('href', '')
        
        # èŽ·å–æ—¶é—´
        time_tag = item.find('span', class_='time')
        published_at = datetime.now()
        if time_tag:
            time_str = time_tag.get_text(strip=True)
            try:
                # æ ¼å¼: 2025-12-04 09:34:07
                published_at = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                pass
        
        # èŽ·å–æè¿°
        desc_tag = item.find('div', class_='miaoshu')
        content = ""
        if desc_tag:
            content = desc_tag.get_text(strip=True)
            # æ¸…ç†HTMLå®žä½“
            content = self._clean_content(content)
        
        # èŽ·å–æ¸¸æˆæ ‡ç­¾
        game_tag = item.find('div', class_='bq')
        game_name = ""
        if game_tag:
            game_link = game_tag.find('a', class_='a')
            if game_link:
                game_name = game_link.get_text(strip=True)
        
        return NewsPost(
            title=title,
            content=content,
            url=url,
            published_at=published_at.isoformat(),
            subreddit=f"3DM/{game_name}" if game_name else "3DM"
        )
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å†…å®¹"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content).strip()
        # ç§»é™¤HTMLå®žä½“æ®‹ç•™
        content = content.replace('&nbsp;', ' ')
        # é™åˆ¶é•¿åº¦
        return content[:1000] if len(content) > 1000 else content
    
    def _is_recent(self, pub_time: datetime, hours: int = 48) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘çš„æ–°é—»ï¼ˆé»˜è®¤48å°æ—¶å†…ï¼‰"""
        if not pub_time:
            return True
        now = datetime.now()
        cutoff = now - timedelta(hours=hours)
        return pub_time >= cutoff

