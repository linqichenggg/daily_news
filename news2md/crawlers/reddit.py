"""
Reddit çˆ¬è™« - ä½¿ç”¨ RSS Feed è·å–æ¸¸æˆæ–°é—»ï¼ˆæ— éœ€ API Keyï¼‰
"""
import re
import html
from datetime import datetime, timezone
from typing import List
from pathlib import Path
import yaml
import feedparser
import requests

from .base import BaseCrawler, NewsPost


class RedditCrawler(BaseCrawler):
    """Reddit RSS çˆ¬è™«"""
    
    RSS_BASE_URL = "https://www.reddit.com/r/{subreddit}/top/.rss?t=day&limit=3"
    # ä½¿ç”¨çœŸå®æµè§ˆå™¨ UA é¿å…è¢« Reddit é™åˆ¶
    USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    def __init__(self, config: dict = None):
        super().__init__("reddit")
        self.config = config or self._load_default_config()
    
    def _load_default_config(self) -> dict:
        """åŠ è½½é»˜è®¤é…ç½®"""
        config_path = Path(__file__).parent.parent / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            full_config = yaml.safe_load(f)
        return full_config.get('reddit', {})
    
    def _get_headers(self) -> dict:
        """è·å–è¯·æ±‚å¤´"""
        return {
            'User-Agent': self.USER_AGENT,
            'Accept': 'application/rss+xml,application/xml,text/xml,*/*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache',
        }
    
    def _fetch_rss(self, url: str) -> feedparser.FeedParserDict:
        """ä½¿ç”¨ requests è·å– RSS å†…å®¹ï¼Œç„¶åç”¨ feedparser è§£æ"""
        response = requests.get(url, headers=self._get_headers(), timeout=30)
        response.raise_for_status()
        return feedparser.parse(response.content)
    
    def test_connection(self) -> bool:
        """æµ‹è¯• RSS è¿æ¥"""
        try:
            test_url = self.RSS_BASE_URL.format(subreddit="Games")
            feed = self._fetch_rss(test_url)
            
            if len(feed.entries) > 0:
                print("âœ… Reddit RSS è¿æ¥æˆåŠŸï¼")
                return True
            else:
                print("âŒ Reddit RSS è¿”å›ç©ºæ•°æ®")
                return False
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                print("âŒ Reddit è¿”å› 403ï¼ŒIP è¢«é™åˆ¶ï¼Œè·³è¿‡ Reddit")
            else:
                print(f"âŒ Reddit HTTP é”™è¯¯: {e.response.status_code}")
            return False
        except Exception as e:
            print(f"âŒ Reddit RSS è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _parse_published_time(self, entry) -> datetime:
        """è§£æå‘å¸ƒæ—¶é—´"""
        # feedparser ä¼šæŠŠæ—¶é—´è§£æåˆ° published_parsed (time.struct_time)
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            from time import mktime
            return datetime.fromtimestamp(mktime(entry.published_parsed), tz=timezone.utc)
        
        # å¤‡ç”¨ï¼šç›´æ¥è§£æå­—ç¬¦ä¸²
        if hasattr(entry, 'published'):
            try:
                # Reddit RSS æ—¶é—´æ ¼å¼ç¤ºä¾‹: "2024-12-03T10:30:00+00:00"
                return datetime.fromisoformat(entry.published.replace('Z', '+00:00'))
            except:
                pass
        
        # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
        return datetime.now(timezone.utc)
    
    def _is_recent(self, pub_time: datetime, hours: int = 48) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯è¿‘æœŸçš„å¸–å­ï¼ˆé»˜è®¤48å°æ—¶å†…ï¼‰"""
        from datetime import timedelta
        now = datetime.now(timezone.utc)
        cutoff = now - timedelta(hours=hours)
        return pub_time >= cutoff
    
    def _extract_content(self, entry) -> str:
        """ä» RSS entry æå–å†…å®¹"""
        content = ""
        
        # å°è¯•è·å–å†…å®¹
        if hasattr(entry, 'content') and entry.content:
            content = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content = entry.summary or ''
        
        # è§£ç  HTML å®ä½“ (&#32; -> ç©ºæ ¼, &quot; -> å¼•å· ç­‰)
        content = html.unescape(content)
        
        # æ¸…ç† HTML æ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # æ¸…ç† "submitted by /u/xxx [link] [comments]" è¿™ç±»å›ºå®šæ–‡æœ¬
        content = re.sub(r'submitted by\s+/u/\S+\s*\[link\]\s*\[comments\]', '', content)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content).strip()
        
        return content[:1000] if content else ""  # é™åˆ¶é•¿åº¦
    
    def _extract_subreddit(self, entry) -> str:
        """ä» entry æå– subreddit åç§°"""
        # ä» link ä¸­æå–: https://www.reddit.com/r/Games/comments/...
        if hasattr(entry, 'link'):
            match = re.search(r'/r/([^/]+)/', entry.link)
            if match:
                return match.group(1)
        return "unknown"
    
    def _to_news_post(self, entry, subreddit: str) -> NewsPost:
        """å°† RSS entry è½¬æ¢ä¸º NewsPost"""
        pub_time = self._parse_published_time(entry)
        
        # è§£ç æ ‡é¢˜ä¸­çš„ HTML å®ä½“
        title = html.unescape(entry.title) if hasattr(entry, 'title') else "æ— æ ‡é¢˜"
        
        return NewsPost(
            title=title,
            content=self._extract_content(entry),
            url=entry.link if hasattr(entry, 'link') else "",
            published_at=pub_time.isoformat(),
            subreddit=subreddit
        )
    
    def crawl_subreddit(self, subreddit_name: str) -> List[NewsPost]:
        """çˆ¬å–å•ä¸ª subreddit"""
        posts = []
        url = self.RSS_BASE_URL.format(subreddit=subreddit_name)
        
        try:
            feed = self._fetch_rss(url)
            
            for entry in feed.entries:
                pub_time = self._parse_published_time(entry)
                
                # åªä¿ç•™48å°æ—¶å†…çš„å¸–å­
                if not self._is_recent(pub_time, hours=48):
                    continue
                
                posts.append(self._to_news_post(entry, subreddit_name))
            
            print(f"  ğŸ“° r/{subreddit_name}: è·å– {len(posts)} æ¡å¸–å­")
            
        except requests.exceptions.HTTPError as e:
            print(f"  âš ï¸ r/{subreddit_name}: HTTP {e.response.status_code}")
        except Exception as e:
            print(f"  âš ï¸ r/{subreddit_name} çˆ¬å–å¤±è´¥: {e}")
        
        return posts
    
    def crawl(self) -> List[NewsPost]:
        """çˆ¬å–æ‰€æœ‰é…ç½®çš„ subreddit"""
        all_posts = []
        subreddits = self.config.get('subreddits', ['Games'])
        
        print(f"\nğŸš€ å¼€å§‹çˆ¬å– Reddit RSSï¼Œå…± {len(subreddits)} ä¸ª subreddit...")
        print(f"   ç­›é€‰æ¡ä»¶: /top?t=day (48å°æ—¶å†…å‘å¸ƒ)")
        
        for subreddit_name in subreddits:
            posts = self.crawl_subreddit(subreddit_name)
            all_posts.extend(posts)
        
        # å»é‡ï¼ˆåŒä¸€å¸–å­å¯èƒ½å‡ºç°åœ¨å¤šä¸ª subredditï¼‰
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            if post.url not in seen_urls:
                seen_urls.add(post.url)
                unique_posts.append(post)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(unique_posts)} æ¡å¸–å­")
        return unique_posts
