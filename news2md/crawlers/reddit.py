"""
Reddit çˆ¬è™« - ä½¿ç”¨ RSS Feed è·å–æ¸¸æˆæ–°é—»ï¼ˆæ— éœ€ API Keyï¼‰
"""
import re
import html
from datetime import datetime, timezone
from typing import List
from pathlib import Path
import yaml
import feedparser

from .base import BaseCrawler, NewsPost


class RedditCrawler(BaseCrawler):
    """Reddit RSS çˆ¬è™«"""
    
    RSS_BASE_URL = "https://www.reddit.com/r/{subreddit}/top/.rss?t=day&limit=3"
    USER_AGENT = "DailyGamingNewsBot/1.0"
    
    def __init__(self, config: dict = None):
        super().__init__("reddit")
        self.config = config or self._load_default_config()
    
    def _load_default_config(self) -> dict:
        """åŠ è½½é»˜è®¤é…ç½®"""
        config_path = Path(__file__).parent.parent / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            full_config = yaml.safe_load(f)
        return full_config.get('reddit', {})
    
    def test_connection(self) -> bool:
        """æµ‹è¯• RSS è¿æ¥"""
        try:
            test_url = self.RSS_BASE_URL.format(subreddit="Games")
            feed = feedparser.parse(test_url, agent=self.USER_AGENT)
            
            if feed.status == 200 and len(feed.entries) > 0:
                print("âœ… Reddit RSS è¿æ¥æˆåŠŸï¼")
                return True
            elif feed.status == 403:
                print("âŒ Reddit è¿”å› 403ï¼Œå¯èƒ½æ˜¯ IP è¢«é™åˆ¶")
                return False
            else:
                print(f"âŒ Reddit RSS è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {feed.status}")
                return False
        except Exception as e:
            print(f"âŒ Reddit RSS è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _parse_published_time(self, entry) -> datetime:
        """è§£æå‘å¸ƒæ—¶é—´"""
        # feedparser ä¼šæŠŠæ—¶é—´è§£æåˆ° published_parsed (time.struct_time)
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            from time import mktime
            return datetime.fromtimestamp(mktime(entry.published_parsed), tz=timezone.utc)
        
        # å¤‡ç”¨ï¼šç›´æ¥è§£æå­—ç¬¦ä¸²
        if hasattr(entry, 'published'):
            try:
                # Reddit RSS æ—¶é—´æ ¼å¼ç¤ºä¾‹: "2024-12-03T10:30:00+00:00"
                return datetime.fromisoformat(entry.published.replace('Z', '+00:00'))
            except:
                pass
        
        # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
        return datetime.now(timezone.utc)
    
    def _is_recent(self, pub_time: datetime, hours: int = 48) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯è¿‘æœŸçš„å¸–å­ï¼ˆé»˜è®¤48å°æ—¶å†…ï¼‰"""
        from datetime import timedelta
        now = datetime.now(timezone.utc)
        cutoff = now - timedelta(hours=hours)
        return pub_time >= cutoff
    
    def _extract_content(self, entry) -> str:
        """ä» RSS entry æå–å†…å®¹"""
        content = ""
        
        # å°è¯•è·å–å†…å®¹
        if hasattr(entry, 'content') and entry.content:
            content = entry.content[0].get('value', '')
        elif hasattr(entry, 'summary'):
            content = entry.summary or ''
        
        # è§£ç  HTML å®ä½“ (&#32; -> ç©ºæ ¼, &quot; -> å¼•å· ç­‰)
        content = html.unescape(content)
        
        # æ¸…ç† HTML æ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # æ¸…ç† "submitted by /u/xxx [link] [comments]" è¿™ç±»å›ºå®šæ–‡æœ¬
        content = re.sub(r'submitted by\s+/u/\S+\s*\[link\]\s*\[comments\]', '', content)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content).strip()
        
        return content[:1000] if content else ""  # é™åˆ¶é•¿åº¦
    
    def _extract_subreddit(self, entry) -> str:
        """ä» entry æå– subreddit åç§°"""
        # ä» link ä¸­æå–: https://www.reddit.com/r/Games/comments/...
        if hasattr(entry, 'link'):
            match = re.search(r'/r/([^/]+)/', entry.link)
            if match:
                return match.group(1)
        return "unknown"
    
    def _to_news_post(self, entry, subreddit: str) -> NewsPost:
        """å°† RSS entry è½¬æ¢ä¸º NewsPost"""
        pub_time = self._parse_published_time(entry)
        
        # è§£ç æ ‡é¢˜ä¸­çš„ HTML å®ä½“
        title = html.unescape(entry.title) if hasattr(entry, 'title') else "æ— æ ‡é¢˜"
        
        return NewsPost(
            title=title,
            content=self._extract_content(entry),
            url=entry.link if hasattr(entry, 'link') else "",
            published_at=pub_time.isoformat(),
            subreddit=subreddit
        )
    
    def crawl_subreddit(self, subreddit_name: str) -> List[NewsPost]:
        """çˆ¬å–å•ä¸ª subreddit"""
        posts = []
        url = self.RSS_BASE_URL.format(subreddit=subreddit_name)
        
        try:
            feed = feedparser.parse(url, agent=self.USER_AGENT)
            
            if feed.status != 200:
                print(f"  âš ï¸ r/{subreddit_name}: çŠ¶æ€ç  {feed.status}")
                return posts
            
            for entry in feed.entries:
                pub_time = self._parse_published_time(entry)
                
                # åªä¿ç•™48å°æ—¶å†…çš„å¸–å­
                if not self._is_recent(pub_time, hours=48):
                    continue
                
                posts.append(self._to_news_post(entry, subreddit_name))
            
            print(f"  ğŸ“° r/{subreddit_name}: è·å– {len(posts)} æ¡å¸–å­")
            
        except Exception as e:
            print(f"  âš ï¸ r/{subreddit_name} çˆ¬å–å¤±è´¥: {e}")
        
        return posts
    
    def crawl(self) -> List[NewsPost]:
        """çˆ¬å–æ‰€æœ‰é…ç½®çš„ subreddit"""
        all_posts = []
        subreddits = self.config.get('subreddits', ['Games'])
        
        print(f"\nğŸš€ å¼€å§‹çˆ¬å– Reddit RSSï¼Œå…± {len(subreddits)} ä¸ª subreddit...")
        print(f"   ç­›é€‰æ¡ä»¶: /top?t=day (48å°æ—¶å†…å‘å¸ƒ)")
        
        for subreddit_name in subreddits:
            posts = self.crawl_subreddit(subreddit_name)
            all_posts.extend(posts)
        
        # å»é‡ï¼ˆåŒä¸€å¸–å­å¯èƒ½å‡ºç°åœ¨å¤šä¸ª subredditï¼‰
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            if post.url not in seen_urls:
                seen_urls.add(post.url)
                unique_posts.append(post)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(unique_posts)} æ¡å¸–å­")
        return unique_posts
