"""
æ–°é—»åˆ†æå™¨ - ä½¿ç”¨ Gemini API åˆ†æå’Œç­›é€‰æ¸¸æˆæ–°é—»
"""
import os
import json
import time
import re
from typing import List, Dict
from pathlib import Path
import yaml
from google import genai


class NewsAnalyzer:
    """ä½¿ç”¨ Gemini åˆ†ææ¸¸æˆæ–°é—»"""
    
    SYSTEM_PROMPT = """ä»å¸–å­ä¸­æå–æ¸¸æˆæ–°é—»ï¼Œè¿”å›JSONæ•°ç»„ï¼š
[{"title":"æ ‡é¢˜15å­—å†…","summary":"è¯¦ç»†æ‘˜è¦100å­—ä»¥å†…ï¼Œå°½å¯èƒ½è¯¦ç»†ä»‹ç»ï¼ŒåŒ…å«èƒŒæ™¯ç»†èŠ‚","audio_text":"æ’­æŠ¥3-5å¥è¯ï¼Œç®€å•ä»‹ç»å³å¯","original_url":"é“¾æ¥"}]
è¦æ±‚ï¼šä¸­æ–‡ã€ç›´æ¥é™ˆè¿°ã€åªè¿”å›JSON"""

    def __init__(self, config: dict = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = config or self._load_default_config()
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        self.model = self.config.get('model', 'gemini-2.5-flash')
        self.max_news = self.config.get('max_news', 5)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆä¿æŒç®€å•ï¼Œå’Œ gemini-api.py ä¸€è‡´ï¼‰
        self.client = genai.Client(api_key=api_key)
    
    def _load_default_config(self) -> dict:
        """åŠ è½½é»˜è®¤é…ç½®"""
        config_path = Path(__file__).parent / "config.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            full_config = yaml.safe_load(f)
        return full_config.get('gemini', {})
    
    def _format_posts(self, posts: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¸–å­æ•°æ®ç”¨äºåˆ†æ"""
        formatted = []
        for i, post in enumerate(posts, 1):
            formatted.append(f"""
--- å¸–å­ {i} ---
æ ‡é¢˜: {post.get('title', '')}
å†…å®¹: {post.get('content', '')[:500]}
æ¥æº: r/{post.get('subreddit', '')}
é“¾æ¥: {post.get('url', '')}
""")
        return "\n".join(formatted)
    
    def analyze(self, posts: List[Dict], batch_size: int = 10) -> List[Dict]:
        """
        åˆ†æ‰¹åˆ†æå¸–å­å¹¶æå–æ–°é—»
        
        Args:
            posts: å¸–å­åˆ—è¡¨ (dict æ ¼å¼)
            batch_size: æ¯æ‰¹å¤„ç†çš„å¸–å­æ•°é‡
            
        Returns:
            æå–çš„æ–°é—»åˆ—è¡¨ï¼ˆæ•´åˆæ‰€æœ‰æ‰¹æ¬¡ï¼‰
        """
        if not posts:
            print("âš ï¸ æ²¡æœ‰å¸–å­éœ€è¦åˆ†æ")
            return []
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        total_posts = len(posts)
        num_batches = (total_posts + batch_size - 1) // batch_size
        
        print(f"\nğŸ¤– ä½¿ç”¨ {self.model} åˆ†æ‰¹åˆ†æ {total_posts} æ¡å¸–å­")
        print(f"   ğŸ“¦ æ¯æ‰¹ {batch_size} æ¡ï¼Œå…± {num_batches} æ‰¹")
        print(f"   ğŸ’¡ æµå¼å“åº”ä¼šå®æ—¶æ˜¾ç¤ºè¿›åº¦\n")
        
        all_news = []
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_posts)
            batch_posts = posts[start_idx:end_idx]
            
            print(f"\n{'='*50}")
            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}ï¼šå¸–å­ {start_idx + 1}-{end_idx}")
            print(f"{'='*50}")
            
            batch_news = self._analyze_batch(batch_posts, batch_idx + 1, num_batches)
            all_news.extend(batch_news)
            
            print(f"   âœ… æœ¬æ‰¹æå– {len(batch_news)} æ¡æ–°é—»")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å… API é™æµ
            if batch_idx < num_batches - 1:
                print(f"   â³ ç­‰å¾… 2 ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(2)
        
        print(f"\n{'='*50}")
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æå– {len(all_news)} æ¡æ–°é—»")
        print(f"{'='*50}")
        
        return all_news
    
    def _analyze_batch(self, posts: List[Dict], batch_num: int, total_batches: int, max_retries: int = 3) -> List[Dict]:
        """
        åˆ†æå•æ‰¹å¸–å­ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            posts: å¸–å­åˆ—è¡¨
            batch_num: å½“å‰æ‰¹æ¬¡å·
            total_batches: æ€»æ‰¹æ¬¡æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        posts_text = self._format_posts(posts)
        
        # æ¯æ‰¹æœ€å¤šæå–çš„æ–°é—»æ•°ï¼ˆæ ¹æ®æ‰¹æ¬¡æ•°é‡åˆ†é…ï¼‰
        news_per_batch = max(2, self.max_news // total_batches + 1)
        
        full_prompt = f"""{self.SYSTEM_PROMPT}

---

è¯·ä»ä»¥ä¸‹ {len(posts)} æ¡å¸–å­ä¸­ï¼Œç­›é€‰å‡ºæœ€å¤š {news_per_batch} æ¡æœ€æœ‰ä»·å€¼çš„å•æœºæ¸¸æˆæ–°é—»ï¼š

{posts_text}

è¯·è¿”å› JSON æ ¼å¼çš„æ–°é—»åˆ—è¡¨ã€‚"""

        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                if attempt > 0:
                    print(f"   ğŸ”„ é‡è¯• {attempt}/{max_retries}...")
                    time.sleep(3)  # é‡è¯•å‰ç­‰å¾… 3 ç§’
                
                # ä½¿ç”¨æµå¼å“åº”
                response_stream = self.client.models.generate_content_stream(
                    model=self.model,
                    contents=full_prompt
                )
                
                full_response = ""
                
                print("   ğŸ“¥ æ¥æ”¶å“åº”: ", end="", flush=True)
                
                for chunk in response_stream:
                    if hasattr(chunk, 'text') and chunk.text:
                        full_response += chunk.text
                        print(".", end="", flush=True)
                
                processing_time = time.time() - start_time
                print(f" å®Œæˆ ({processing_time:.1f}ç§’)")
                
                # è§£æ JSON
                return self._parse_json(full_response)
                
            except Exception as e:
                print(f" âŒ å¤±è´¥")
                print(f"   é”™è¯¯: {type(e).__name__}: {str(e)[:100]}")
                
                if attempt < max_retries - 1:
                    print(f"   â³ ç­‰å¾… 5 ç§’åé‡è¯•æ‰¹æ¬¡ {batch_num}...")
                    time.sleep(5)
                else:
                    print(f"   âš ï¸ æ‰¹æ¬¡ {batch_num} å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡")
                    return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸å½±å“å…¶ä»–æ‰¹æ¬¡
        
        return []
    
    def _parse_json(self, response: str) -> List[Dict]:
        """è§£æ JSON å“åº”"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except json.JSONDecodeError:
                    pass
            print(f"âš ï¸ æ— æ³•è§£æ JSON å“åº”")
            return []

